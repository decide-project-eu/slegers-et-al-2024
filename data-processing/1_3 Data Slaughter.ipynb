{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be7293e-8676-4958-8e40-930527c7bc73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "#fill in directory and key\n",
    "spark.sparkContext.hadoopConfiguration.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d273d8-072e-4985-8768-2436e809f143",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row, Column\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, DoubleType, TimestampType, FloatType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9964a3-4575-4282-a040-f2b4d0e59572",
     "showTitle": true,
     "title": "UDF to correct breed names"
    }
   },
   "outputs": [],
   "source": [
    "# 'hubbard' is used for slow-growing hubbard breeds (hubbard ja87 and ja57). Other fast-growing hubbards are called 'hubbard conventional' or 'hubbard flex'.\n",
    "# 'ross' is used for fast-growing ross breeds. 'ross ranger' is called 'rowan ranger' everywhere to separate them from other ross breeds.\n",
    "# 'ross 308' -> 'ross', 'cobb 500' -> 'cobb'\n",
    "\n",
    "import re\n",
    "\n",
    "def breed_correction(text):\n",
    "    dict = {\n",
    "    \"bijproducten\" : \"byproduct\",\n",
    "    \"fokproduct\" : \"breeding product\",\n",
    "    \"conventioneel\" : \"conventional\",\n",
    "    \"87 \" : \"\",\n",
    "    \"57\" : \"\",\n",
    "    \" ff\" : \"\",\n",
    "    \" 308\" : \"\",\n",
    "    \" 500\": \"\"} \n",
    "    \n",
    "    #if value is Null, do nothing\n",
    "    if text is None:\n",
    "        val = None\n",
    "    else:\n",
    "        # First make text lower case\n",
    "        lowerCaseText = text.lower()\n",
    "        \n",
    "        # Create a regular expression  from the dictionary keys\n",
    "        regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "        # For each match, look-up corresponding value in dictionary\n",
    "        lowerCaseText = regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], lowerCaseText) \n",
    "        \n",
    "        # Then replace specific strings with the correct breed name:\n",
    "        if re.search(\"ranger\", lowerCaseText):\n",
    "            val = 'rowan ranger'\n",
    "        elif lowerCaseText == \"kippen diverse rassen\":\n",
    "            val = \"diverse breeds\"\n",
    "        elif lowerCaseText == \"hubbard ja\":\n",
    "            val = \"hubbard\"\n",
    "        else:\n",
    "            val = lowerCaseText\n",
    "    return val\n",
    "\n",
    "breed_correction_udf = udf(breed_correction, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3648512-2c90-423e-9ead-7f2f8006aa28",
     "showTitle": true,
     "title": "UDF to correct pen names"
    }
   },
   "outputs": [],
   "source": [
    "def pen_correction(text):\n",
    "    dict = {\n",
    "    \" \" : \"\",\n",
    "    \"-\" : \"\"} \n",
    "    \n",
    "    #if value is Null, do nothing\n",
    "    if text is not None:\n",
    "        # Create a regular expression  from the dictionary keys\n",
    "        regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "        # For each match, look-up corresponding value in dictionary\n",
    "        text = regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) \n",
    "        text = text.lower()\n",
    "    return text\n",
    "\n",
    "pen_correction_udf = udf(pen_correction, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e09afdd-9611-4f07-b432-563aa973ddac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "Data cleaning/filter steps:\n",
    "- Exclude entries with Roosters or Hens (instead of Unsexed)\n",
    "- Only include Type 'Kip' (no ducks or Null)\n",
    "- Exclude CompanyType 'Leghanen'\n",
    "- HatchDate between 2013-2021   (! then null values are excluded)\n",
    "- SlaughterDate between 2013-2021\n",
    "- SlaughterDate > HatchDate\n",
    "- Remove entries with Unsexed (number slaughtered) == 1\n",
    "- Remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080e6a1e-ddee-42e6-a0c0-3be1851aa493",
     "showTitle": true,
     "title": "Data Afvoermeldingen"
    }
   },
   "outputs": [],
   "source": [
    "dfAfvoer = spark \\\n",
    "  .read \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"delimiter\", \";\") \\\n",
    "  .option(\"decimal\", \",\") \\\n",
    "  .option(\"multiLine\", \"true\") \\\n",
    "  .option(\"nullValue\", \"-\") \\\n",
    "  .csv(\"//afvoermeldingen.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca404c3d-5ae5-4623-bb98-656f1b7d184c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# change column names\n",
    "# correct breed and pen names\n",
    "# drop unnecessary columns\n",
    "\n",
    "dfSlaughterWithoutFarmId1 = dfAfvoer \\\n",
    "    .withColumnRenamed('Datumverplaatsing','MoveDate') \\\n",
    "    .withColumnRenamed('Opmerking','Note') \\\n",
    "    .withColumnRenamed('Verplaatsingtype','MoveType') \\\n",
    "    .withColumnRenamed('Geboorte-datum','HatchDate') \\\n",
    "    .withColumnRenamed('Slachtdatum','SlaughterDate') \\\n",
    "    .withColumnRenamed('Kipnrherk','KIPNumber') \\\n",
    "    .withColumnRenamed('Kipnrbest','KIPNumberDestination') \\\n",
    "    .withColumnRenamed('Cate-gorie','Category') \\\n",
    "    .withColumn('Breed', breed_correction_udf(F.col('Ras'))) \\\n",
    "    .withColumn('PenNumber', pen_correction_udf(F.col('Stal'))) \\\n",
    "    .drop('Stal') \\\n",
    "    .withColumnRenamed('Soort','Type') \\\n",
    "    .withColumnRenamed('Houderij-vorm','CompanyType') \\\n",
    "    .withColumnRenamed('Levering-type','DeliveryType') \\\n",
    "    .withColumnRenamed('Hanen','Roosters') \\\n",
    "    .withColumnRenamed('Hennen','Hens') \\\n",
    "    .withColumnRenamed('Ongesext','Unsexed') \\\n",
    "    .withColumnRenamed('Gewicht(kg)','Weight') \\\n",
    "    .withColumnRenamed('Mortaliteit (%)','Mortality') \\\n",
    "    .withColumnRenamed('Uitvoerder-monitoring','MonitoringExecutor') \\\n",
    "    .withColumnRenamed('Voetzool-laesies','FootpadLesionScores') \\\n",
    "    .withColumnRenamed('Bedrijfssoortbestemming','DestinationCompanyType') \\\n",
    "    .drop('Formulier','Versie','Import','Export','Pluim-veetype','Bedrijfssoortherkomst','Ras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a06c903-0233-4c48-9a52-b0635c37abcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# change data types\n",
    "# add age\n",
    "\n",
    "dfSlaughterWithoutFarmId = dfSlaughterWithoutFarmId1 \\\n",
    "    .withColumn('Unsexed',\n",
    "                dfSlaughterWithoutFarmId1['Unsexed'] \\\n",
    "                .cast(\"integer\")) \\\n",
    "    .withColumn('Weight',\n",
    "                F.when(F.col('Weight').cast('integer') > 10,\n",
    "                      F.col('Weight').cast('integer'))) \\\n",
    "    .withColumn('Mortality',\n",
    "                F.regexp_replace(dfSlaughterWithoutFarmId1['Mortality'], ',', '.')\n",
    "                .cast(\"float\")) \\\n",
    "    .withColumn('FootpadLesionScores',\n",
    "                dfSlaughterWithoutFarmId1['FootpadLesionScores'] \\\n",
    "                .cast(\"integer\")) \\\n",
    "    .withColumn('MoveDate',\n",
    "                F.to_timestamp(dfSlaughterWithoutFarmId1['MoveDate'], \"dd-MM-yyyy\")) \\\n",
    "    .withColumn('HatchDate',\n",
    "                F.to_timestamp(dfSlaughterWithoutFarmId1['HatchDate'], \"dd-MM-yyyy\")) \\\n",
    "    .withColumn('SlaughterDate',\n",
    "                F.to_timestamp(dfSlaughterWithoutFarmId1['SlaughterDate'], \"dd-MM-yyyy\")) \\\n",
    "    .withColumn('AgeAtSlaughter',\n",
    "                F.datediff(F.col('MoveDate'), F.col('HatchDate'))) \\\n",
    "    .withColumnRenamed('KIPNumber', 'PoultryFarmIdentification2') \\\n",
    "    .withColumnRenamed('PenNumber', 'House2') \\\n",
    "    .withColumnRenamed('HatchDate', 'HatchDateKIP') \\\n",
    "    .distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e12e2b94-ddee-4635-af11-5a5f62719fd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Add PMP data with UBN\n",
    "\n",
    "And VetId, thinning yes/no etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2070c346-6a40-4fbf-ba96-b69a31175035",
     "showTitle": false,
     "title": "Test: add PMP data here"
    }
   },
   "outputs": [],
   "source": [
    "dfPMPFlocks1 = spark.read.parquet(\"//dfPMPFlocks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c48f5e7-bb9d-4db8-8208-4fad41402226",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfPMPFlocks = dfPMPFlocks1 \\\n",
    "    .filter(F.col('HatchDate') <= \"2021-12-31 00:00:00\") \\\n",
    "    .withColumn('PoultryFarmIdentification',\n",
    "                F.substring(F.col('PoultryFarmIdentification'), 3, 5)) \\\n",
    "    .dropDuplicates(['FlockIdentification', 'House', 'NumberPlaced'])\n",
    "\n",
    "# there are some duplicate flockIDs. This is because of a double VetID: these are filtered because otherwise the flock and therefore the number placed is doubled. 50 flocks have two different houses: these are kept because I see them as separate flocks (see notebook 'merge PMP data')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993970fb-f140-4d58-9855-ac4cd56ec540",
     "showTitle": true,
     "title": "Add Postal Code"
    }
   },
   "outputs": [],
   "source": [
    "dfHouder = spark. \\\n",
    "  read. \\\n",
    "  option(\"header\", \"true\"). \\\n",
    "  option(\"delimiter\", \",\"). \\\n",
    "  csv(\"//vmp_houder.csv\")\n",
    "\n",
    "dfFarmer = dfHouder \\\n",
    "    .withColumnRenamed('HDRID','FarmerIdentification') \\\n",
    "    .withColumn('PoultryFarmIdentification', F.substring(F.col('KIPNUMMER'), 3, 5)) \\\n",
    "    .withColumnRenamed('UBN','FarmIdentification') \\\n",
    "    .withColumnRenamed('POSTCODE', 'PostalCode') \\\n",
    "    .drop('WOONPLAATS', 'ADRES', 'KIPBEGINDATUM', 'KIPNUMMER', 'NAAM', 'DATUM_LAMU', 'USER_LAMU', 'KIPEINDDATUM', 'PMPKUBID', 'STRAAT', 'HUISNR', 'HUISNRTOEV', 'REGISTRATIENUMMER', 'POSTPLAATS', 'POSTADRES', 'POSTPOSTCODE', 'POSTSTRAAT', 'POSTHUISNR', 'POSTHUISNRTOEV', 'SRTCODE', 'PMPKIPID', 'POSTCODE', 'BEDRIJFSOORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df22e51-c1de-44e5-bbe5-7f3f7c17de0b",
     "showTitle": true,
     "title": "Combine dataframes"
    }
   },
   "outputs": [],
   "source": [
    "cond = [dfSlaughterWithoutFarmId.PoultryFarmIdentification2 == dfPMPFlocks.PoultryFarmIdentification,\n",
    "        dfSlaughterWithoutFarmId.House2 == dfPMPFlocks.House,\n",
    "        F.abs(F.datediff(dfSlaughterWithoutFarmId.HatchDateKIP, dfPMPFlocks.HatchDate)) <= 3]\n",
    "\n",
    "dfHatch_PMP  = dfSlaughterWithoutFarmId \\\n",
    "    .join(dfPMPFlocks, on = cond, how = 'inner') \\\n",
    "    .join(dfFarmer, on = ['FarmerIdentification', 'FarmIdentification', 'PoultryFarmIdentification'], how = \"inner\") \\\n",
    "    .drop('House2', 'PoultryFarmIdentification2') \\\n",
    "    .filter((F.col('Roosters') == '0') &\n",
    "           (F.col('Hens') == '0') &\n",
    "           (F.col('Type') == 'Kip') &\n",
    "           (F.col('CompanyType') != 'Leghanen')) \\\n",
    "    .filter((F.col('Unsexed') > 1)) \\\n",
    "    .filter((F.col('HatchDate') >= \"2013-01-01 00:00:00\") &\n",
    "            (F.col('HatchDate') <= \"2021-12-31 00:00:00\")) \\\n",
    "    .filter((F.col('MoveDate') >= \"2013-01-01 00:00:00\") &\n",
    "            (F.col('MoveDate') <= \"2021-12-31 00:00:00\") &\n",
    "            (F.col('AgeAtSlaughter') > 1) &\n",
    "            (F.col('AgeAtSlaughter') < 100)) \\\n",
    "    .drop('Roosters', 'Hens')\n",
    "\n",
    "# this is without dropDuplicates on flock information, so all slaughter and thinning transports are a separate record.\n",
    "# dfFarmer join gives same number of rows (no duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c82129c-84de-4ec0-b579-3866e087afd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transform to events\n",
    "- Hatch\n",
    "- Thinning\n",
    "- Slaughter\n",
    "- Death\n",
    "- Relocation\n",
    "\n",
    "Thanks to the filters, there are no more Null Dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "337e5c38-340a-451d-9558-5a7a39d73db4",
     "showTitle": true,
     "title": "Seperate thinning, slaughter, death and relocation"
    }
   },
   "outputs": [],
   "source": [
    "dfThinningTransport = dfHatch_PMP \\\n",
    "    .filter((F.col('DeliveryType') == \"Uitladen\") &\n",
    "            (F.col('MoveType') == \"Afvoer\"))\n",
    "\n",
    "dfSlaughterTransport = dfHatch_PMP \\\n",
    "    .filter((F.col('DeliveryType') == \"Wegladen\") &\n",
    "            (F.col('MoveType') == \"Afvoer\"))\n",
    "\n",
    "dfDeathTransport = dfHatch_PMP \\\n",
    "    .filter(F.col('MoveType') == \"Uitval \")\n",
    "\n",
    "dfRelocationTransport = dfHatch_PMP \\\n",
    "    .filter(F.col('MoveType') == \"Overplaatsing\")\n",
    "\n",
    "# Note: thinning and slaughter are filtered on MoveDate (like the rest), not SlaughterDate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa95c1c-5e51-4176-971b-0238d416c315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hatch records:\n",
    "# not only from slaughter as sometimes slaughter is accidentally recorded as thinning, plus some flocks only have death/relocation\n",
    "\n",
    "# distinct on: FarmerIdentification, FarmIdentification, PoultryFarmIdentification, FlockIdentification, NumberPlaced, HatchDateKIP or HatchDate (???)\n",
    "# one hatch date per PMP registration, so all columns from PMP\n",
    "\n",
    "dfHatch = dfHatch_PMP \\\n",
    "    .dropDuplicates([\"House\", \"FlockIdentification\", \"NumberPlaced\", \"HatchDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4333b05d-4531-44b1-9ea5-d6e09c9c337c",
     "showTitle": true,
     "title": "Transform hatch date to Event"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# fill in for the specific dataset:\n",
    "Type = \"Hatch\"\n",
    "Pen = \"House\"\n",
    "EventDate = \"HatchDate\"\n",
    "df = dfHatch\n",
    "# Hatch date is approximately placement date\n",
    "\n",
    "RightColumns = [\"FarmIdentification\",\n",
    "                Pen,\n",
    "                EventDate,\n",
    "                \"EventType\"]\n",
    "OtherColumns = sorted(list(set(df.columns) - set(RightColumns)))\n",
    "\n",
    "#create column EventDate and Type, put all other columns in metadata column, drop all other columns\n",
    "dfHatchEventsStruct = df \\\n",
    "    .withColumnRenamed(EventDate, \"EventDate\") \\\n",
    "    .withColumnRenamed(Pen, \"Pen\") \\\n",
    "    .withColumn(\"EventType\", F.lit(Type)) \\\n",
    "    .withColumn(\"MetaData\", F.struct(*OtherColumns)) \\\n",
    "    .select(\"FarmIdentification\",\"Pen\",\"EventDate\",\"EventType\", \"MetaData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5316929-5070-4e58-8bf6-57c7b624ffce",
     "showTitle": true,
     "title": "Transform Thinning to Events"
    }
   },
   "outputs": [],
   "source": [
    "# fill in for the specific dataset:\n",
    "Type = \"Thinning\"\n",
    "Pen = \"House\"\n",
    "EventDate = \"MoveDate\"\n",
    "df = dfThinningTransport\n",
    "\n",
    "RightColumns = [\"FarmIdentification\",\n",
    "                Pen,\n",
    "                EventDate,\n",
    "                \"EventType\"]\n",
    "OtherColumns = sorted(list(set(df.columns) - set(RightColumns)))\n",
    "\n",
    "#create column EventDate and Type, put all other columns in metadata column, drop all other columns\n",
    "dfThinningEventsStruct = df \\\n",
    "    .withColumnRenamed(EventDate, \"EventDate\") \\\n",
    "    .withColumnRenamed(Pen, \"Pen\") \\\n",
    "    .withColumn(\"EventType\", F.lit(Type)) \\\n",
    "    .withColumn(\"MetaData\", F.struct(*OtherColumns)) \\\n",
    "    .select(\"FarmIdentification\",\"Pen\",\"EventDate\",\"EventType\", \"MetaData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ceb9eda-dca9-4905-bcd3-960d40a7a375",
     "showTitle": true,
     "title": "Transform Slaughter to Events"
    }
   },
   "outputs": [],
   "source": [
    "# fill in for the specific dataset:\n",
    "Type = \"Slaughter\"\n",
    "Pen = \"House\"\n",
    "EventDate = \"MoveDate\"\n",
    "# --> MoveDate and SlaughterDate should be the same.*\n",
    "df = dfSlaughterTransport\n",
    "\n",
    "RightColumns = [\"FarmIdentification\",\n",
    "                Pen,\n",
    "                EventDate,\n",
    "                \"EventType\"]\n",
    "OtherColumns = sorted(list(set(df.columns) - set(RightColumns)))\n",
    "\n",
    "# create column EventDate and Type, put all other columns in metadata column, drop all other columns\n",
    "dfSlaughterEventsStruct = df \\\n",
    "    .withColumnRenamed(EventDate, \"EventDate\") \\\n",
    "    .withColumnRenamed(Pen, \"Pen\") \\\n",
    "    .withColumn(\"EventType\", F.lit(Type)) \\\n",
    "    .withColumn(\"Metadata\", F.struct(*OtherColumns)) \\\n",
    "    .select(\"FarmIdentification\",\"Pen\",\"EventDate\",\"EventType\", \"MetaData\")\n",
    "\n",
    "# * movedate is not the same as slaughterdate in 10.103 cases of 136.586 slaughters. 8.512 of those have a difference of 1 day. (this is from unfiltered excel). In a lot of the others, either the movedate or the slaughterdate has a wrong year (before Hatch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0040b35-165d-4ccb-8893-4b2f6f926906",
     "showTitle": true,
     "title": "Transform Death to Events"
    }
   },
   "outputs": [],
   "source": [
    "# fill in for the specific dataset:\n",
    "Type = \"Death\"\n",
    "Pen = \"House\"\n",
    "EventDate = \"MoveDate\"\n",
    "df = dfRelocationTransport\n",
    "\n",
    "RightColumns = [\"FarmIdentification\",\n",
    "                Pen,\n",
    "                EventDate,\n",
    "                \"EventType\"]\n",
    "OtherColumns = sorted(list(set(df.columns) - set(RightColumns)))\n",
    "\n",
    "# create column EventDate and Type, put all other columns in metadata column, drop all other columns\n",
    "dfDeathEventsStruct = df \\\n",
    "    .withColumnRenamed(EventDate, \"EventDate\") \\\n",
    "    .withColumnRenamed(Pen, \"Pen\") \\\n",
    "    .withColumn(\"EventType\", F.lit(Type)) \\\n",
    "    .withColumn(\"Metadata\", F.struct(*OtherColumns)) \\\n",
    "    .select(\"FarmIdentification\",\"Pen\",\"EventDate\",\"EventType\", \"MetaData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa74ca75-70f2-443c-86b9-0f57e0561279",
     "showTitle": true,
     "title": "Transform Relocation to Events"
    }
   },
   "outputs": [],
   "source": [
    "# fill in for the specific dataset:\n",
    "Type = \"Relocation\"\n",
    "Pen = \"House\"\n",
    "EventDate = \"MoveDate\"\n",
    "df = dfDeathTransport\n",
    "\n",
    "RightColumns = [\"FarmIdentification\",\n",
    "                Pen,\n",
    "                EventDate,\n",
    "                \"EventType\"]\n",
    "OtherColumns = sorted(list(set(df.columns) - set(RightColumns)))\n",
    "\n",
    "# create column EventDate and Type, put all other columns in metadata column, drop all other columns\n",
    "dfRelocationEventsStruct = df \\\n",
    "    .withColumnRenamed(EventDate, \"EventDate\") \\\n",
    "    .withColumnRenamed(Pen, \"Pen\") \\\n",
    "    .withColumn(\"EventType\", F.lit(Type)) \\\n",
    "    .withColumn(\"Metadata\", F.struct(*OtherColumns)) \\\n",
    "    .select(\"FarmIdentification\",\"Pen\",\"EventDate\",\"EventType\", \"MetaData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5fb06c-be90-4b41-aebc-95caf129c70c",
     "showTitle": true,
     "title": "Save Event Dataframes"
    }
   },
   "outputs": [],
   "source": [
    "dfHatchEventsStruct.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"//dfHatchEventsStruct.parquet\")\n",
    "\n",
    "# first version: 24/11/22  (this was first not saved separately)\n",
    "# edits:\n",
    "# - 25/11/22 added pen number correction\n",
    "# - 29/11/22 removed error: column 'Stal'\n",
    "# - 12/12/22 hubbard ja -> hubbard\n",
    "# - 13/01/22 filter on number slaughtered >1, removed weights <10\n",
    "# - 30/08/23 changed 'birth' to 'hatch'\n",
    "# - 20/09/23 complete change (PMP)\n",
    "# - 06/02/24 added hatch dates from thinning/death/relocation, removed double PMP flocks with >1 VetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982c5ab5-09da-4370-85f5-9080bd4c5882",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfSlaughterEventsStruct.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"//dfSlaughterEventsStruct.parquet\")\n",
    "\n",
    "# first version: 24/11/22  (this was first not saved seperately)\n",
    "# edits:\n",
    "# - 25/11/22 added pen number correction\n",
    "# - 29/11/22 removed error: column 'Stal'\n",
    "# - 12/12/22 hubbard ja -> hubbard\n",
    "# - 13/01/22 filter on number slaughtered >1, removed weights <10\n",
    "# - 30/08/23 changed 'birth' to 'hatch' and 'FootPadLesions' to 'footpadlesions'\n",
    "# - 20/09/23 complete change (PMP)\n",
    "# - 06/02/24 removed double PMP flocks with >1 VetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6441f146-2cad-489e-8f7c-957904be75e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfThinningEventsStruct.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"//dfThinningEventsStruct.parquet\")\n",
    "\n",
    "# first version: 24/11/22  (this was first not saved seperately)\n",
    "# edits:\n",
    "# - 25/11/22 added pen number correction\n",
    "# - 29/11/22 removed error: column 'Stal'\n",
    "# - 12/12/22 hubbard ja -> hubbard\n",
    "# - 13/01/22 filter on number slaughtered >1, removed weights <10\n",
    "# - 30/08/23 changed 'birth' to 'hatch'\n",
    "# - 20/09/23 complete change (PMP)\n",
    "# - 06/02/24 removed double PMP flocks with >1 VetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f40555d-c610-4826-b2e8-ef1720561251",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfDeathEventsStruct.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"//dfDeathEventsStruct.parquet\")\n",
    "\n",
    "# first version: 24/11/22  (this was first not saved seperately)\n",
    "# edits:\n",
    "# - 25/11/22 added pen number correction\n",
    "# - 29/11/22 removed error: column 'Stal'\n",
    "# - 12/12/22 hubbard ja -> hubbard\n",
    "# - 13/01/22 filter on number slaughtered >1, removed weights <10\n",
    "# - 30/08/23 changed 'birth' to 'hatch'\n",
    "# - 20/09/23 complete change (PMP)\n",
    "# - 06/02/24 removed double PMP flocks with >1 VetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3043a41-2cd0-4cd9-891a-000296487eae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfRelocationEventsStruct.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"//dfRelocationEventsStruct.parquet\")\n",
    "\n",
    "# first version: 24/11/22  (this was first not saved seperately)\n",
    "# edits:\n",
    "# - 25/11/22 added pen number correction\n",
    "# - 29/11/22 removed error: column 'Stal'\n",
    "# - 12/12/22 hubbard ja -> hubbard\n",
    "# - 13/01/22 filter on number slaughtered >1, removed weights <10\n",
    "# - 30/08/23 changed 'birth' to 'hatch'\n",
    "# - 20/09/23 complete change (PMP)\n",
    "# - 06/02/24 removed double PMP flocks with >1 VetID"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2809803259911055,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_3 Data Slaughter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
